{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyternotify extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyternotify\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from logging import StreamHandler\n",
    "from logging import INFO, WARNING\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "handler = StreamHandler()\n",
    "handler.setLevel(INFO)\n",
    "logger.setLevel(INFO)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = \"train\"\n",
    "VAL = \"val\"\n",
    "TEST = \"test\"\n",
    "PHASES = [TRAIN, VAL]\n",
    "\n",
    "SAVE_PATH = os.path.join(os.getcwd(), \"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_formatted_time(elapsed_time, msg=\"\"):\n",
    "    minutes, seconds = map(int, divmod(elapsed_time, 60))\n",
    "    print(\"Elapsed time - {0}: {1}min {2}s\".format(msg, minutes, seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_name=\"\"):\n",
    "    model_path = os.path.join(SAVE_PATH, model_name)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "def load_model(model_name):\n",
    "    model = Net().to(device)\n",
    "    model_path = os.path.join(SAVE_PATH, model_name)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_log_path():\n",
    "    # return os.path.join(\"logs\", \"train_{}\".format(datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")))\n",
    "    return os.path.join(\"logs\", \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "test_batch_size = 256\n",
    "\n",
    "# default\n",
    "params = {\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 10,\n",
    "    \"lr\": 0.001,\n",
    "    \"momentum\": 0.9,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_datasets(data_transforms):\n",
    "    \"\"\"画像前処理変更したら呼ぶ\"\"\"\n",
    "    image_datasets = {phase: torchvision.datasets.CIFAR10(root=\"../../data\",\n",
    "                                                                                                       train=phase is \"train\",\n",
    "                                                                                                       download=True,\n",
    "                                                                                                       transform=data_transforms[phase])\n",
    "                                      for phase in PHASES}\n",
    "    dataset_sizes = {phase: len(image_datasets[phase]) for phase in PHASES}\n",
    "    return image_datasets, dataset_sizes\n",
    "\n",
    "def init_dataloaders(batch_size, image_datasets):\n",
    "    \"\"\"バッチサイズ変更したら呼ぶ\"\"\"\n",
    "    return {phase: torch.utils.data.DataLoader(image_datasets[phase],\n",
    "                                                                               batch_size=batch_size,\n",
    "                                                                               shuffle=True,\n",
    "                                                                               num_workers=4)\n",
    "                 for phase in [\"train\", \"val\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "}\n",
    "\n",
    "image_datasets, dataset_sizes = init_datasets(data_transforms)\n",
    "dataloaders = init_dataloaders(params[\"batch_size\"], image_datasets)\n",
    "\n",
    "classes = (\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(dataloaders[TRAIN])\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(\" \".join(\"%5s\" % classes[labels[j]] for j in range(params[\"batch_size\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Convolution Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from net import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net,  self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,  16,  5)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.MaxPool2d(2,  2)\n",
    "        self.conv2 = nn.Conv2d(16,  16,  5)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5,  120)\n",
    "        self.fc2 = nn.Linear(120,  84)\n",
    "        self.fc3 = nn.Linear(84,  10)\n",
    "\n",
    "    def forward(self,  x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # 32x32x3 ->  28x28x6 ->14x14x6\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  #  ->10x10x16 -> 5x5x16\n",
    "        x = x.view(-1,  16 * 5 * 5)  # -> 400\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, scheduler, params):\n",
    "    since = time.time()\n",
    "    \n",
    "    # Tensorboard\n",
    "    log_path = generate_log_path()\n",
    "    writer = SummaryWriter(log_path)\n",
    "    \n",
    "    epochs = params[\"epochs\"]\n",
    "\n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = dict()\n",
    "        epoch_acc = dict()\n",
    "        \n",
    "        for phase in PHASES:\n",
    "            if phase == TRAIN:\n",
    "                is_train = True\n",
    "                scheduler.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                is_train = False\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for batch_idx, (inputs, labels) in enumerate(dataloaders[phase], 0):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if is_train:\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss[phase] = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc[phase] = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            if phase == VAL and epoch_acc[phase] > best_acc:\n",
    "                best_acc = epoch_acc[phase]\n",
    "                best_model_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print(\"Epoch {}/{}\\tTrain Loss: {:.4f} Acc: {:.4f}\\tVal Loss: {:.4f} Acc: {:.4f}\".format(\n",
    "            epoch,\n",
    "            epochs - 1,\n",
    "            epoch_loss[TRAIN],\n",
    "            epoch_acc[TRAIN],\n",
    "            epoch_loss[VAL],\n",
    "            epoch_acc[VAL]\n",
    "        ))\n",
    "        writer.add_scalars(\n",
    "            \"losses\",\n",
    "            {\n",
    "                \"train_loss\": epoch_loss[TRAIN],\n",
    "                \"val_loss\": epoch_loss[VAL],\n",
    "            },\n",
    "            epoch\n",
    "        )\n",
    "        writer.add_scalars(\n",
    "            \"acc\",\n",
    "            {\n",
    "                \"train_acc\": epoch_acc[TRAIN],\n",
    "                \"val_acc\": epoch_acc[VAL]\n",
    "            },\n",
    "            epoch\n",
    "        )\n",
    "\n",
    "    print()\n",
    "    print(\"Best val Acc: {:4f}\".format(best_acc))\n",
    "    \n",
    "    writer.close()\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    \n",
    "    print()\n",
    "    display_formatted_time(time.time() - since)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(model):\n",
    "    \"\"\"ラベルごとの精度を算出\"\"\"\n",
    "    class_correct = [0. for i in range(len(classes))]\n",
    "    class_total = [0. for i in range(len(classes))]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaders[VAL]:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    for i in range(10):\n",
    "        print(\"Accuracy of\\t%5s:\\t%2d %%\" % (classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 0/49\tTrain Loss: 1.8440 Acc: 0.3370\tVal Loss: 1.6088 Acc: 0.4023\n",
      "Epoch 1/49\tTrain Loss: 1.4035 Acc: 0.4877\tVal Loss: 1.3513 Acc: 0.5073\n",
      "Epoch 2/49\tTrain Loss: 1.2661 Acc: 0.5415\tVal Loss: 1.2507 Acc: 0.5483\n",
      "Epoch 3/49\tTrain Loss: 1.1900 Acc: 0.5730\tVal Loss: 1.2773 Acc: 0.5451\n",
      "Epoch 4/49\tTrain Loss: 1.1387 Acc: 0.5917\tVal Loss: 1.1703 Acc: 0.5860\n",
      "Epoch 5/49\tTrain Loss: 1.1071 Acc: 0.6066\tVal Loss: 1.1114 Acc: 0.6057\n",
      "Epoch 6/49\tTrain Loss: 1.0812 Acc: 0.6154\tVal Loss: 1.1397 Acc: 0.6019\n",
      "Epoch 7/49\tTrain Loss: 1.0625 Acc: 0.6253\tVal Loss: 1.0928 Acc: 0.6152\n",
      "Epoch 8/49\tTrain Loss: 1.0520 Acc: 0.6277\tVal Loss: 1.0763 Acc: 0.6174\n",
      "Epoch 9/49\tTrain Loss: 1.0435 Acc: 0.6314\tVal Loss: 1.0721 Acc: 0.6220\n",
      "Epoch 10/49\tTrain Loss: 1.0440 Acc: 0.6289\tVal Loss: 1.0719 Acc: 0.6223\n",
      "Epoch 11/49\tTrain Loss: 1.0431 Acc: 0.6307\tVal Loss: 1.0711 Acc: 0.6226\n",
      "Epoch 12/49\tTrain Loss: 1.0435 Acc: 0.6297\tVal Loss: 1.0677 Acc: 0.6226\n",
      "Epoch 13/49\tTrain Loss: 1.0404 Acc: 0.6312\tVal Loss: 1.0684 Acc: 0.6223\n",
      "Epoch 14/49\tTrain Loss: 1.0376 Acc: 0.6328\tVal Loss: 1.0572 Acc: 0.6281\n",
      "Epoch 15/49\tTrain Loss: 1.0316 Acc: 0.6343\tVal Loss: 1.0597 Acc: 0.6270\n",
      "Epoch 16/49\tTrain Loss: 1.0184 Acc: 0.6414\tVal Loss: 1.0701 Acc: 0.6271\n",
      "Epoch 17/49\tTrain Loss: 1.0072 Acc: 0.6427\tVal Loss: 1.1340 Acc: 0.6019\n",
      "Epoch 18/49\tTrain Loss: 0.9860 Acc: 0.6512\tVal Loss: 1.0073 Acc: 0.6502\n",
      "Epoch 19/49\tTrain Loss: 0.9728 Acc: 0.6580\tVal Loss: 1.0143 Acc: 0.6400\n",
      "Epoch 20/49\tTrain Loss: 0.9469 Acc: 0.6655\tVal Loss: 0.9865 Acc: 0.6501\n",
      "Epoch 21/49\tTrain Loss: 0.9316 Acc: 0.6732\tVal Loss: 1.0379 Acc: 0.6394\n",
      "Epoch 22/49\tTrain Loss: 0.9116 Acc: 0.6792\tVal Loss: 1.0483 Acc: 0.6369\n",
      "Epoch 23/49\tTrain Loss: 0.8960 Acc: 0.6864\tVal Loss: 0.9734 Acc: 0.6606\n",
      "Epoch 24/49\tTrain Loss: 0.8761 Acc: 0.6920\tVal Loss: 0.9216 Acc: 0.6785\n",
      "Epoch 25/49\tTrain Loss: 0.8579 Acc: 0.7003\tVal Loss: 0.9158 Acc: 0.6775\n",
      "Epoch 26/49\tTrain Loss: 0.8476 Acc: 0.7046\tVal Loss: 0.9410 Acc: 0.6729\n",
      "Epoch 27/49\tTrain Loss: 0.8385 Acc: 0.7066\tVal Loss: 0.9008 Acc: 0.6825\n",
      "Epoch 28/49\tTrain Loss: 0.8321 Acc: 0.7086\tVal Loss: 0.8945 Acc: 0.6876\n",
      "Epoch 29/49\tTrain Loss: 0.8254 Acc: 0.7113\tVal Loss: 0.8908 Acc: 0.6874\n",
      "Epoch 30/49\tTrain Loss: 0.8252 Acc: 0.7118\tVal Loss: 0.8906 Acc: 0.6869\n",
      "Epoch 31/49\tTrain Loss: 0.8268 Acc: 0.7118\tVal Loss: 0.8908 Acc: 0.6890\n",
      "Epoch 32/49\tTrain Loss: 0.8252 Acc: 0.7107\tVal Loss: 0.8950 Acc: 0.6882\n",
      "Epoch 33/49\tTrain Loss: 0.8282 Acc: 0.7104\tVal Loss: 0.9016 Acc: 0.6847\n",
      "Epoch 34/49\tTrain Loss: 0.8323 Acc: 0.7099\tVal Loss: 0.9219 Acc: 0.6760\n",
      "Epoch 35/49\tTrain Loss: 0.8305 Acc: 0.7089\tVal Loss: 0.9118 Acc: 0.6813\n",
      "Epoch 36/49\tTrain Loss: 0.8358 Acc: 0.7068\tVal Loss: 0.9809 Acc: 0.6613\n",
      "Epoch 37/49\tTrain Loss: 0.8420 Acc: 0.7038\tVal Loss: 0.9646 Acc: 0.6651\n",
      "Epoch 38/49\tTrain Loss: 0.8340 Acc: 0.7067\tVal Loss: 0.9506 Acc: 0.6755\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"batch_size\": 1024,\n",
    "    \"epochs\": 50,\n",
    "    \"lr\": 0.001,\n",
    "    \"momentum\": 0.9,\n",
    "}\n",
    "\n",
    "image_datasets, dataset_sizes = init_datasets(data_transforms=data_transforms)\n",
    "dataloaders = init_dataloaders(params[\"batch_size\"], image_datasets=image_datasets)\n",
    "model = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(),\n",
    "#                                          lr=params[\"lr\"],\n",
    "#                                          momentum=params[\"momentum\"])\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                                            lr=params[\"lr\"])\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer=optimizer,\n",
    "                                                                              T_max=10,  # Maximum number of iterations\n",
    "                                                                              eta_min=0,  # 最小学習率\n",
    "                                                                              last_epoch=-1)  # The index of last epoch\n",
    "\n",
    "model = train(model, criterion, optimizer, scheduler, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of\tplane:\t67 %\n",
      "Accuracy of\t  car:\t77 %\n",
      "Accuracy of\t bird:\t49 %\n",
      "Accuracy of\t  cat:\t37 %\n",
      "Accuracy of\t deer:\t54 %\n",
      "Accuracy of\t  dog:\t56 %\n",
      "Accuracy of\t frog:\t76 %\n",
      "Accuracy of\thorse:\t69 %\n",
      "Accuracy of\t ship:\t75 %\n",
      "Accuracy of\ttruck:\t75 %\n"
     ]
    }
   ],
   "source": [
    "calc_acc(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(\"original_Adam_epoch50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"original_SGD_epoch50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 100,\n",
    "    \"lr\": 0.001,\n",
    "    \"momentum\": 0.9,\n",
    "}\n",
    "\n",
    "dataloaders = init_dataloaders(batch_size)\n",
    "model = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                                            lr=params[\"lr\"])\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer=optimizer,\n",
    "                                                                              T_max=10,  # Maximum number of iterations\n",
    "                                                                              eta_min=0,  # 最小学習率\n",
    "                                                                              last_epoch=-1)  # The index of last epoch\n",
    "\n",
    "model = train(model, criterion, optimizer, scheduler, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_acc(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"batch_size\": 1024,\n",
    "    \"epochs\": 50,\n",
    "    \"lr\": 0.001,\n",
    "    \"momentum\": 0.9,\n",
    "}\n",
    "\n",
    "dataloaders = init_dataloaders(batch_size)\n",
    "model = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                                            lr=params[\"lr\"])\n",
    "scheduler = lr_scheduler.StepLR(optimizer=optimizer,\n",
    "                                                     step_size=10,\n",
    "                                                     gamma=0.9)\n",
    "\n",
    "model = train(model, criterion, optimizer, scheduler, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_acc(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning ResNet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "}\n",
    "\n",
    "image_datasets, dataset_sizes = init_datasets(data_transforms)\n",
    "dataloaders = init_dataloaders(params[\"batch_size\"], image_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root=\"../../data\",\n",
    "                                                                       train=True,\n",
    "                                                                       download=True,\n",
    "                                                                       transform=transforms.Compose([\n",
    "                                                                           transforms.RandomResizedCrop(224),\n",
    "                                                                           transforms.RandomHorizontalFlip(),\n",
    "                                                                           transforms.ToTensor(),\n",
    "                                                                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                                                       ]))\n",
    "dataloader[TRAIN] = torch.utils.data.DataLoader(trainset,\n",
    "                                                                           batch_size=batch_size,\n",
    "                                                                           shuffle=True,\n",
    "                                                                           num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=\"../../data\",\n",
    "                                                                      train=False,\n",
    "                                                                      download=True,\n",
    "                                                                       transform=transforms.Compose([\n",
    "                                                                           transforms.Resize(256),\n",
    "                                                                           transforms.RandomResizedCrop(224),\n",
    "                                                                           transforms.ToTensor(),\n",
    "                                                                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                                                       ]))\n",
    "test_loader = torch.utils.data.DataLoader(testset,\n",
    "                                                                          batch_size=test_batch_size,\n",
    "                                                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 10,\n",
    "    \"lr\": 0.001,\n",
    "    \"momentum\": 0.9,\n",
    "}\n",
    "\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "}\n",
    "\n",
    "image_datasets, dataset_sizes = init_datasets(data_transforms)\n",
    "dataloaders = init_dataloaders(params[\"batch_size\"], image_datasets)\n",
    "\n",
    "# Load pretrained model\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.conv1 = nn.Conv2d(in_channels=3,\n",
    "                                                    out_channels=64,\n",
    "                                                    kernel_size=7,\n",
    "                                                    stride=2,\n",
    "                                                    padding=3,\n",
    "                                                    bias=False)\n",
    "model_ft.fc = nn.Linear(num_ftrs, 10)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_ft.parameters(),\n",
    "                                            lr=params[\"lr\"])\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer=optimizer,\n",
    "                                                                              T_max=10,  # Maximum number of iterations\n",
    "                                                                              eta_min=0,  # 最小学習率\n",
    "                                                                              last_epoch=-1)  # The index of last epoch\n",
    "\n",
    "model_ft = train(model_ft, criterion, optimizer, scheduler, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for (inputs, labels) in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_ft(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print(\"Accuracy of the network on the 10000 test images: %d %%\" % (100 * correct / total))\n",
    "\n",
    "display_formatted_time(time.time() - since)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.conv1 = nn.Conv2d(in_channels=3,\n",
    "                                                    out_channels=64,\n",
    "                                                    kernel_size=7,\n",
    "                                                    stride=2,\n",
    "                                                    padding=3,\n",
    "                                                    bias=False)\n",
    "model_ft.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "print(model_ft)\n",
    "\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(),\n",
    "                                              lr=lr,\n",
    "                                              momentum=momentum)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(dataloader[TRAIN], 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model_ft(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        exp_lr_scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % log_interval == (log_interval - 1):\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tRunningLoss: {:.3f}\".format(\n",
    "                epoch, batch_idx * len(inputs), len(dataloader[TRAIN].dataset),\n",
    "                100. * batch_idx / len(dataloader[TRAIN]), loss.item(), running_loss / log_interval\n",
    "            ))\n",
    "            running_loss = 0.0\n",
    "\n",
    "display_formatted_time(time.time() - since)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for (inputs, labels) in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_ft(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print(\"Accuracy of the network on the 10000 test images: %d %%\" % (100 * correct / total))\n",
    "\n",
    "display_formatted_time(time.time() - since)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.vgg16(pretrained=True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model_ft.classifier[6].in_features\n",
    "\n",
    "model_ft.classifier[6] = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "print(model_ft)\n",
    "\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(),\n",
    "                                              lr=lr,\n",
    "                                              momentum=momentum)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(dataloader[TRAIN], 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model_ft(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        exp_lr_scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % log_interval == (log_interval - 1):\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tRunningLoss: {:.3f}\".format(\n",
    "                epoch, batch_idx * len(inputs), len(dataloader[TRAIN].dataset),\n",
    "                100. * batch_idx / len(dataloader[TRAIN]), loss.item(), running_loss / log_interval\n",
    "            ))\n",
    "            running_loss = 0.0\n",
    "\n",
    "display_formatted_time(time.time() - since)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for (inputs, labels) in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_ft(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print(\"Accuracy of the network on the 10000 test images: %d %%\" % (100 * correct / total))\n",
    "\n",
    "display_formatted_time(time.time() - since)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
