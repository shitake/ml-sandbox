{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from logging import StreamHandler\n",
    "from logging import INFO, WARNING\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "handler = StreamHandler()\n",
    "handler.setLevel(INFO)\n",
    "logger.setLevel(INFO)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = \"train\"\n",
    "VAL = \"val\"\n",
    "TEST = \"test\"\n",
    "PHASES = [TRAIN, VAL]\n",
    "\n",
    "SAVE_PATH = os.path.join(os.getcwd(), \"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_formatted_time(elapsed_time, msg=\"\"):\n",
    "    minutes, seconds = map(int, divmod(elapsed_time, 60))\n",
    "    print(\"Elapsed time - {0}: {1}min {2}s\".format(msg, minutes, seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_name=\"\"):\n",
    "    model_path = os.path.join(SAVE_PATH, model_name)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "def load_model(model_name):\n",
    "    model = Net().to(device)\n",
    "    model_path = os.path.join(SAVE_PATH, model_name)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_log_path():\n",
    "    # return os.path.join(\"logs\", \"train_{}\".format(datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")))\n",
    "    return os.path.join(\"logs\", \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "test_batch_size = 256\n",
    "\n",
    "# default\n",
    "params = {\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 10,\n",
    "    \"lr\": 0.001,\n",
    "    \"momentum\": 0.9,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_datasets(data_transforms):\n",
    "    \"\"\"画像前処理変更したら呼ぶ\"\"\"\n",
    "    image_datasets = {phase: torchvision.datasets.CIFAR10(root=\"../../data\",\n",
    "                                                                                                       train=phase is \"train\",\n",
    "                                                                                                       download=True,\n",
    "                                                                                                       transform=data_transforms[phase])\n",
    "                                      for phase in PHASES}\n",
    "    dataset_sizes = {phase: len(image_datasets[phase]) for phase in PHASES}\n",
    "    return image_datasets, dataset_sizes\n",
    "\n",
    "def init_dataloaders(batch_size, image_datasets):\n",
    "    \"\"\"バッチサイズ変更したら呼ぶ\"\"\"\n",
    "    return {phase: torch.utils.data.DataLoader(image_datasets[phase],\n",
    "                                                                               batch_size=batch_size,\n",
    "                                                                               shuffle=True,\n",
    "                                                                               num_workers=4)\n",
    "                 for phase in PHASES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "}\n",
    "\n",
    "image_datasets, dataset_sizes = init_datasets(data_transforms)\n",
    "dataloaders = init_dataloaders(params[\"batch_size\"], image_datasets)\n",
    "\n",
    "classes = (\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(dataloaders[TRAIN])\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(\" \".join(\"%5s\" % classes[labels[j]] for j in range(params[\"batch_size\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Convolution Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from net import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net,  self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,  16,  5)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.MaxPool2d(2,  2)\n",
    "        self.conv2 = nn.Conv2d(16,  16,  5)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5,  120)\n",
    "        self.fc2 = nn.Linear(120,  84)\n",
    "        self.fc3 = nn.Linear(84,  10)\n",
    "\n",
    "    def forward(self,  x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # 32x32x3 ->  28x28x6 ->14x14x6\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  #  ->10x10x16 -> 5x5x16\n",
    "        x = x.view(-1,  16 * 5 * 5)  # -> 400\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, scheduler, params):\n",
    "    since = time.time()\n",
    "    \n",
    "    # Tensorboard\n",
    "    log_path = generate_log_path()\n",
    "    writer = SummaryWriter(log_path)\n",
    "    \n",
    "    epochs = params[\"epochs\"]\n",
    "\n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = dict()\n",
    "        epoch_acc = dict()\n",
    "        \n",
    "        for phase in PHASES:\n",
    "            if phase == TRAIN:\n",
    "                is_train = True\n",
    "                scheduler.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                is_train = False\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for batch_idx, (inputs, labels) in enumerate(dataloaders[phase], 0):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if is_train:\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss[phase] = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc[phase] = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            if phase == VAL and epoch_acc[phase] > best_acc:\n",
    "                best_acc = epoch_acc[phase]\n",
    "                best_model_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print(\"Epoch {}/{}\\tLoss Train: {:.4f} Val: {:.4f}\\tAcc Train: {:.4f} Val: {:.4f}\".format(\n",
    "            epoch,\n",
    "            epochs - 1,\n",
    "            epoch_loss[TRAIN],\n",
    "            epoch_loss[VAL],\n",
    "            epoch_acc[TRAIN],\n",
    "            epoch_acc[VAL]\n",
    "        ))\n",
    "        writer.add_scalars(\n",
    "            \"losses\",\n",
    "            {\n",
    "                \"train_loss\": epoch_loss[TRAIN],\n",
    "                \"val_loss\": epoch_loss[VAL],\n",
    "            },\n",
    "            epoch\n",
    "        )\n",
    "        writer.add_scalars(\n",
    "            \"acc\",\n",
    "            {\n",
    "                \"train_acc\": epoch_acc[TRAIN],\n",
    "                \"val_acc\": epoch_acc[VAL]\n",
    "            },\n",
    "            epoch\n",
    "        )\n",
    "\n",
    "    print()\n",
    "    print(\"Best val Acc: {:4f}\".format(best_acc))\n",
    "    \n",
    "    writer.close()\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    gc.collect()\n",
    "    \n",
    "    print()\n",
    "    display_formatted_time(time.time() - since)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(model):\n",
    "    \"\"\"ラベルごとの精度を算出\"\"\"\n",
    "    class_correct = [0. for i in range(len(classes))]\n",
    "    class_total = [0. for i in range(len(classes))]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaders[VAL]:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    for i in range(10):\n",
    "        print(\"Accuracy of\\t%5s:\\t%2d %%\" % (classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_optimizer(model, params, algo=\"Adam\"):\n",
    "    if algo == \"Adam\":\n",
    "        return optim.Adam(model.parameters(),\n",
    "                                          lr=params[\"lr\"],\n",
    "                                          weight_decay=params[\"weight_decay\"])\n",
    "    elif algo ==\"SGD\":\n",
    "        return optim.SGD(model.parameters(),\n",
    "                                        lr=params[\"lr\"],\n",
    "                                        momentum=params[\"momentum\"])\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scheduler(optimizer, method=\"cos\"):\n",
    "    if method == \"cos\":\n",
    "        return lr_scheduler.CosineAnnealingLR(optimizer=optimizer,\n",
    "                                                                            T_max=10,  # Maximum number of iterations\n",
    "                                                                            eta_min=0,  # 最小学習率\n",
    "                                                                            last_epoch=-1)  # The index of last epoch\n",
    "    elif method == \"step\":\n",
    "        return  lr_scheduler.StepLR(optimizer,\n",
    "                                                         step_size=7,\n",
    "                                                         gamma=0.1)\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 0/49\tLoss Train: 1.8130 Val: 1.4782\tAcc Train: 0.3172 Val: 0.4590\n",
      "Epoch 1/49\tLoss Train: 1.4296 Val: 1.2875\tAcc Train: 0.4757 Val: 0.5315\n",
      "Epoch 2/49\tLoss Train: 1.2963 Val: 1.2037\tAcc Train: 0.5360 Val: 0.5609\n",
      "Epoch 3/49\tLoss Train: 1.2263 Val: 1.1444\tAcc Train: 0.5629 Val: 0.5878\n",
      "Epoch 4/49\tLoss Train: 1.1743 Val: 1.1139\tAcc Train: 0.5833 Val: 0.5991\n",
      "Epoch 5/49\tLoss Train: 1.1375 Val: 1.0737\tAcc Train: 0.5992 Val: 0.6146\n",
      "Epoch 6/49\tLoss Train: 1.1051 Val: 1.0566\tAcc Train: 0.6108 Val: 0.6182\n",
      "Epoch 7/49\tLoss Train: 1.0812 Val: 1.0391\tAcc Train: 0.6178 Val: 0.6276\n",
      "Epoch 8/49\tLoss Train: 1.0739 Val: 1.0275\tAcc Train: 0.6224 Val: 0.6334\n",
      "Epoch 9/49\tLoss Train: 1.0685 Val: 1.0253\tAcc Train: 0.6260 Val: 0.6329\n",
      "Epoch 10/49\tLoss Train: 1.0662 Val: 1.0252\tAcc Train: 0.6254 Val: 0.6332\n",
      "Epoch 11/49\tLoss Train: 1.0635 Val: 1.0244\tAcc Train: 0.6259 Val: 0.6337\n",
      "Epoch 12/49\tLoss Train: 1.0677 Val: 1.0227\tAcc Train: 0.6229 Val: 0.6349\n",
      "Epoch 13/49\tLoss Train: 1.0662 Val: 1.0339\tAcc Train: 0.6250 Val: 0.6312\n",
      "Epoch 14/49\tLoss Train: 1.0609 Val: 1.0214\tAcc Train: 0.6270 Val: 0.6372\n",
      "Epoch 15/49\tLoss Train: 1.0537 Val: 1.0402\tAcc Train: 0.6271 Val: 0.6260\n",
      "Epoch 16/49\tLoss Train: 1.0474 Val: 0.9939\tAcc Train: 0.6331 Val: 0.6513\n",
      "Epoch 17/49\tLoss Train: 1.0301 Val: 1.0106\tAcc Train: 0.6393 Val: 0.6412\n",
      "Epoch 18/49\tLoss Train: 1.0127 Val: 0.9884\tAcc Train: 0.6463 Val: 0.6498\n",
      "Epoch 19/49\tLoss Train: 0.9932 Val: 0.9564\tAcc Train: 0.6527 Val: 0.6616\n",
      "Epoch 20/49\tLoss Train: 0.9706 Val: 0.9737\tAcc Train: 0.6632 Val: 0.6553\n",
      "Epoch 21/49\tLoss Train: 0.9473 Val: 0.9280\tAcc Train: 0.6694 Val: 0.6773\n",
      "Epoch 22/49\tLoss Train: 0.9237 Val: 0.9113\tAcc Train: 0.6803 Val: 0.6793\n",
      "Epoch 23/49\tLoss Train: 0.8941 Val: 0.9048\tAcc Train: 0.6873 Val: 0.6812\n",
      "Epoch 24/49\tLoss Train: 0.8842 Val: 0.8692\tAcc Train: 0.6950 Val: 0.6942\n",
      "Epoch 25/49\tLoss Train: 0.8625 Val: 0.8622\tAcc Train: 0.6997 Val: 0.6961\n",
      "Epoch 26/49\tLoss Train: 0.8445 Val: 0.8524\tAcc Train: 0.7083 Val: 0.7033\n",
      "Epoch 27/49\tLoss Train: 0.8298 Val: 0.8351\tAcc Train: 0.7134 Val: 0.7115\n",
      "Epoch 28/49\tLoss Train: 0.8178 Val: 0.8328\tAcc Train: 0.7161 Val: 0.7103\n",
      "Epoch 29/49\tLoss Train: 0.8188 Val: 0.8274\tAcc Train: 0.7165 Val: 0.7115\n",
      "Epoch 30/49\tLoss Train: 0.8092 Val: 0.8267\tAcc Train: 0.7198 Val: 0.7117\n",
      "Epoch 31/49\tLoss Train: 0.8096 Val: 0.8268\tAcc Train: 0.7177 Val: 0.7116\n",
      "Epoch 32/49\tLoss Train: 0.8154 Val: 0.8310\tAcc Train: 0.7184 Val: 0.7111\n",
      "Epoch 33/49\tLoss Train: 0.8186 Val: 0.8332\tAcc Train: 0.7195 Val: 0.7084\n",
      "Epoch 34/49\tLoss Train: 0.8270 Val: 0.8459\tAcc Train: 0.7132 Val: 0.7045\n",
      "Epoch 35/49\tLoss Train: 0.8306 Val: 0.8574\tAcc Train: 0.7121 Val: 0.7026\n",
      "Epoch 36/49\tLoss Train: 0.8262 Val: 0.8807\tAcc Train: 0.7138 Val: 0.6908\n",
      "Epoch 37/49\tLoss Train: 0.8317 Val: 0.8358\tAcc Train: 0.7113 Val: 0.7095\n",
      "Epoch 38/49\tLoss Train: 0.8208 Val: 0.8783\tAcc Train: 0.7165 Val: 0.6940\n",
      "Epoch 39/49\tLoss Train: 0.8160 Val: 0.8236\tAcc Train: 0.7183 Val: 0.7168\n",
      "Epoch 40/49\tLoss Train: 0.8111 Val: 0.8595\tAcc Train: 0.7208 Val: 0.7009\n",
      "Epoch 41/49\tLoss Train: 0.7994 Val: 0.8492\tAcc Train: 0.7249 Val: 0.7060\n",
      "Epoch 42/49\tLoss Train: 0.7806 Val: 0.8439\tAcc Train: 0.7280 Val: 0.7075\n",
      "Epoch 43/49\tLoss Train: 0.7719 Val: 0.8073\tAcc Train: 0.7329 Val: 0.7194\n",
      "Epoch 44/49\tLoss Train: 0.7569 Val: 0.8175\tAcc Train: 0.7387 Val: 0.7210\n",
      "Epoch 45/49\tLoss Train: 0.7378 Val: 0.8082\tAcc Train: 0.7462 Val: 0.7250\n",
      "Epoch 46/49\tLoss Train: 0.7235 Val: 0.7885\tAcc Train: 0.7493 Val: 0.7285\n",
      "Epoch 47/49\tLoss Train: 0.7075 Val: 0.7832\tAcc Train: 0.7566 Val: 0.7309\n",
      "Epoch 48/49\tLoss Train: 0.7035 Val: 0.7750\tAcc Train: 0.7561 Val: 0.7328\n",
      "Epoch 49/49\tLoss Train: 0.6994 Val: 0.7720\tAcc Train: 0.7589 Val: 0.7336\n",
      "\n",
      "Best val Acc: 0.733600\n",
      "\n",
      "Elapsed time - : 2min 14s\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 50,\n",
    "    \"lr\": 0.001,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 1e-5,\n",
    "}\n",
    "\n",
    "image_datasets, dataset_sizes = init_datasets(data_transforms=data_transforms)\n",
    "dataloaders = init_dataloaders(params[\"batch_size\"], image_datasets=image_datasets)\n",
    "model = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = generate_optimizer(model, params, \"Adam\")\n",
    "scheduler = generate_scheduler(optimizer, \"cos\")\n",
    "\n",
    "model = train(model, criterion, optimizer, scheduler, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of\tplane:\t77 %\n",
      "Accuracy of\t  car:\t85 %\n",
      "Accuracy of\t bird:\t59 %\n",
      "Accuracy of\t  cat:\t52 %\n",
      "Accuracy of\t deer:\t70 %\n",
      "Accuracy of\t  dog:\t62 %\n",
      "Accuracy of\t frog:\t82 %\n",
      "Accuracy of\thorse:\t79 %\n",
      "Accuracy of\t ship:\t83 %\n",
      "Accuracy of\ttruck:\t80 %\n"
     ]
    }
   ],
   "source": [
    "calc_acc(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"original_Adam_epoch50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"original_SGD_epoch50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 0/199\tTrain Loss: 2.0250 Acc: 0.2619\tVal Loss: 1.9228 Acc: 0.3014\n",
      "Epoch 1/199\tTrain Loss: 1.5971 Acc: 0.4066\tVal Loss: 1.6279 Acc: 0.3907\n",
      "Epoch 2/199\tTrain Loss: 1.4276 Acc: 0.4755\tVal Loss: 1.4042 Acc: 0.4819\n",
      "Epoch 3/199\tTrain Loss: 1.3346 Acc: 0.5152\tVal Loss: 1.3171 Acc: 0.5187\n",
      "Epoch 4/199\tTrain Loss: 1.2696 Acc: 0.5433\tVal Loss: 1.2726 Acc: 0.5378\n",
      "Epoch 5/199\tTrain Loss: 1.2269 Acc: 0.5576\tVal Loss: 1.2141 Acc: 0.5627\n",
      "Epoch 6/199\tTrain Loss: 1.1959 Acc: 0.5707\tVal Loss: 1.1913 Acc: 0.5759\n",
      "Epoch 7/199\tTrain Loss: 1.1735 Acc: 0.5813\tVal Loss: 1.1807 Acc: 0.5779\n",
      "Epoch 8/199\tTrain Loss: 1.1618 Acc: 0.5844\tVal Loss: 1.1698 Acc: 0.5807\n",
      "Epoch 9/199\tTrain Loss: 1.1559 Acc: 0.5865\tVal Loss: 1.1662 Acc: 0.5832\n",
      "Epoch 10/199\tTrain Loss: 1.1524 Acc: 0.5882\tVal Loss: 1.1660 Acc: 0.5830\n",
      "Epoch 11/199\tTrain Loss: 1.1520 Acc: 0.5882\tVal Loss: 1.1654 Acc: 0.5850\n",
      "Epoch 12/199\tTrain Loss: 1.1524 Acc: 0.5869\tVal Loss: 1.1636 Acc: 0.5823\n",
      "Epoch 13/199\tTrain Loss: 1.1469 Acc: 0.5905\tVal Loss: 1.1551 Acc: 0.5861\n",
      "Epoch 14/199\tTrain Loss: 1.1377 Acc: 0.5943\tVal Loss: 1.1476 Acc: 0.5886\n",
      "Epoch 15/199\tTrain Loss: 1.1211 Acc: 0.6003\tVal Loss: 1.1386 Acc: 0.5949\n",
      "Epoch 16/199\tTrain Loss: 1.1051 Acc: 0.6070\tVal Loss: 1.1234 Acc: 0.6007\n",
      "Epoch 17/199\tTrain Loss: 1.0795 Acc: 0.6170\tVal Loss: 1.1042 Acc: 0.6076\n",
      "Epoch 18/199\tTrain Loss: 1.0617 Acc: 0.6239\tVal Loss: 1.0745 Acc: 0.6136\n",
      "Epoch 19/199\tTrain Loss: 1.0278 Acc: 0.6368\tVal Loss: 1.0943 Acc: 0.6156\n",
      "Epoch 20/199\tTrain Loss: 1.0183 Acc: 0.6413\tVal Loss: 1.0794 Acc: 0.6213\n",
      "Epoch 21/199\tTrain Loss: 0.9927 Acc: 0.6494\tVal Loss: 1.0841 Acc: 0.6100\n",
      "Epoch 22/199\tTrain Loss: 0.9763 Acc: 0.6569\tVal Loss: 1.0423 Acc: 0.6355\n",
      "Epoch 23/199\tTrain Loss: 0.9599 Acc: 0.6610\tVal Loss: 1.0052 Acc: 0.6526\n",
      "Epoch 24/199\tTrain Loss: 0.9478 Acc: 0.6680\tVal Loss: 0.9966 Acc: 0.6489\n",
      "Epoch 25/199\tTrain Loss: 0.9302 Acc: 0.6736\tVal Loss: 0.9678 Acc: 0.6622\n",
      "Epoch 26/199\tTrain Loss: 0.9187 Acc: 0.6782\tVal Loss: 0.9666 Acc: 0.6640\n",
      "Epoch 27/199\tTrain Loss: 0.9079 Acc: 0.6811\tVal Loss: 0.9548 Acc: 0.6641\n",
      "Epoch 28/199\tTrain Loss: 0.9018 Acc: 0.6822\tVal Loss: 0.9480 Acc: 0.6703\n",
      "Epoch 29/199\tTrain Loss: 0.8998 Acc: 0.6856\tVal Loss: 0.9446 Acc: 0.6698\n",
      "Epoch 30/199\tTrain Loss: 0.9005 Acc: 0.6862\tVal Loss: 0.9447 Acc: 0.6710\n",
      "Epoch 31/199\tTrain Loss: 0.8982 Acc: 0.6850\tVal Loss: 0.9450 Acc: 0.6705\n",
      "Epoch 32/199\tTrain Loss: 0.9013 Acc: 0.6851\tVal Loss: 0.9531 Acc: 0.6696\n",
      "Epoch 33/199\tTrain Loss: 0.9029 Acc: 0.6841\tVal Loss: 0.9508 Acc: 0.6707\n",
      "Epoch 34/199\tTrain Loss: 0.9008 Acc: 0.6846\tVal Loss: 0.9597 Acc: 0.6642\n",
      "Epoch 35/199\tTrain Loss: 0.9046 Acc: 0.6817\tVal Loss: 0.9565 Acc: 0.6656\n",
      "Epoch 36/199\tTrain Loss: 0.9049 Acc: 0.6819\tVal Loss: 0.9460 Acc: 0.6717\n",
      "Epoch 37/199\tTrain Loss: 0.9012 Acc: 0.6835\tVal Loss: 1.0080 Acc: 0.6469\n",
      "Epoch 38/199\tTrain Loss: 0.8943 Acc: 0.6863\tVal Loss: 0.9947 Acc: 0.6576\n",
      "Epoch 39/199\tTrain Loss: 0.8894 Acc: 0.6871\tVal Loss: 0.9766 Acc: 0.6542\n",
      "Epoch 40/199\tTrain Loss: 0.8787 Acc: 0.6917\tVal Loss: 0.9523 Acc: 0.6678\n",
      "Epoch 41/199\tTrain Loss: 0.8688 Acc: 0.6925\tVal Loss: 0.9685 Acc: 0.6643\n",
      "Epoch 42/199\tTrain Loss: 0.8608 Acc: 0.6970\tVal Loss: 0.9269 Acc: 0.6794\n",
      "Epoch 43/199\tTrain Loss: 0.8472 Acc: 0.7022\tVal Loss: 0.9134 Acc: 0.6838\n",
      "Epoch 44/199\tTrain Loss: 0.8404 Acc: 0.7043\tVal Loss: 0.9424 Acc: 0.6736\n",
      "Epoch 45/199\tTrain Loss: 0.8244 Acc: 0.7105\tVal Loss: 0.8966 Acc: 0.6883\n",
      "Epoch 46/199\tTrain Loss: 0.8176 Acc: 0.7123\tVal Loss: 0.8917 Acc: 0.6905\n",
      "Epoch 47/199\tTrain Loss: 0.8150 Acc: 0.7142\tVal Loss: 0.8954 Acc: 0.6915\n",
      "Epoch 48/199\tTrain Loss: 0.8083 Acc: 0.7152\tVal Loss: 0.8844 Acc: 0.6961\n",
      "Epoch 49/199\tTrain Loss: 0.8058 Acc: 0.7184\tVal Loss: 0.8791 Acc: 0.6956\n",
      "Epoch 50/199\tTrain Loss: 0.8033 Acc: 0.7189\tVal Loss: 0.8792 Acc: 0.6946\n",
      "Epoch 51/199\tTrain Loss: 0.8037 Acc: 0.7173\tVal Loss: 0.8793 Acc: 0.6953\n",
      "Epoch 52/199\tTrain Loss: 0.8057 Acc: 0.7194\tVal Loss: 0.8809 Acc: 0.6942\n",
      "Epoch 53/199\tTrain Loss: 0.8102 Acc: 0.7155\tVal Loss: 0.8835 Acc: 0.6928\n",
      "Epoch 54/199\tTrain Loss: 0.8105 Acc: 0.7160\tVal Loss: 0.9008 Acc: 0.6879\n",
      "Epoch 55/199\tTrain Loss: 0.8138 Acc: 0.7147\tVal Loss: 0.8973 Acc: 0.6885\n",
      "Epoch 56/199\tTrain Loss: 0.8147 Acc: 0.7122\tVal Loss: 0.8876 Acc: 0.6942\n",
      "Epoch 57/199\tTrain Loss: 0.8116 Acc: 0.7141\tVal Loss: 0.9043 Acc: 0.6853\n",
      "Epoch 58/199\tTrain Loss: 0.8200 Acc: 0.7107\tVal Loss: 0.9319 Acc: 0.6776\n",
      "Epoch 59/199\tTrain Loss: 0.8096 Acc: 0.7158\tVal Loss: 0.8978 Acc: 0.6882\n",
      "Epoch 60/199\tTrain Loss: 0.8100 Acc: 0.7143\tVal Loss: 0.9465 Acc: 0.6696\n",
      "Epoch 61/199\tTrain Loss: 0.7968 Acc: 0.7202\tVal Loss: 0.8782 Acc: 0.6953\n",
      "Epoch 62/199\tTrain Loss: 0.7900 Acc: 0.7230\tVal Loss: 0.8701 Acc: 0.6986\n",
      "Epoch 63/199\tTrain Loss: 0.7792 Acc: 0.7283\tVal Loss: 0.8688 Acc: 0.7022\n",
      "Epoch 64/199\tTrain Loss: 0.7704 Acc: 0.7282\tVal Loss: 0.8706 Acc: 0.6977\n",
      "Epoch 65/199\tTrain Loss: 0.7626 Acc: 0.7341\tVal Loss: 0.8744 Acc: 0.6979\n",
      "Epoch 66/199\tTrain Loss: 0.7539 Acc: 0.7358\tVal Loss: 0.8544 Acc: 0.7052\n",
      "Epoch 67/199\tTrain Loss: 0.7497 Acc: 0.7362\tVal Loss: 0.8464 Acc: 0.7082\n",
      "Epoch 68/199\tTrain Loss: 0.7446 Acc: 0.7388\tVal Loss: 0.8424 Acc: 0.7094\n",
      "Epoch 69/199\tTrain Loss: 0.7407 Acc: 0.7424\tVal Loss: 0.8411 Acc: 0.7094\n",
      "Epoch 70/199\tTrain Loss: 0.7410 Acc: 0.7414\tVal Loss: 0.8411 Acc: 0.7103\n",
      "Epoch 71/199\tTrain Loss: 0.7391 Acc: 0.7408\tVal Loss: 0.8406 Acc: 0.7095\n",
      "Epoch 72/199\tTrain Loss: 0.7404 Acc: 0.7409\tVal Loss: 0.8430 Acc: 0.7098\n",
      "Epoch 73/199\tTrain Loss: 0.7450 Acc: 0.7382\tVal Loss: 0.8428 Acc: 0.7106\n",
      "Epoch 74/199\tTrain Loss: 0.7445 Acc: 0.7392\tVal Loss: 0.8452 Acc: 0.7097\n",
      "Epoch 75/199\tTrain Loss: 0.7502 Acc: 0.7356\tVal Loss: 0.8629 Acc: 0.7021\n",
      "Epoch 76/199\tTrain Loss: 0.7499 Acc: 0.7377\tVal Loss: 0.8592 Acc: 0.7039\n",
      "Epoch 77/199\tTrain Loss: 0.7556 Acc: 0.7348\tVal Loss: 0.8829 Acc: 0.6947\n",
      "Epoch 78/199\tTrain Loss: 0.7562 Acc: 0.7329\tVal Loss: 0.9084 Acc: 0.6868\n",
      "Epoch 79/199\tTrain Loss: 0.7524 Acc: 0.7363\tVal Loss: 0.9081 Acc: 0.6880\n",
      "Epoch 80/199\tTrain Loss: 0.7495 Acc: 0.7364\tVal Loss: 0.8727 Acc: 0.6991\n",
      "Epoch 81/199\tTrain Loss: 0.7465 Acc: 0.7356\tVal Loss: 0.8479 Acc: 0.7078\n",
      "Epoch 82/199\tTrain Loss: 0.7375 Acc: 0.7402\tVal Loss: 0.9049 Acc: 0.6917\n",
      "Epoch 83/199\tTrain Loss: 0.7304 Acc: 0.7439\tVal Loss: 0.8503 Acc: 0.7084\n",
      "Epoch 84/199\tTrain Loss: 0.7206 Acc: 0.7456\tVal Loss: 0.8380 Acc: 0.7127\n",
      "Epoch 85/199\tTrain Loss: 0.7099 Acc: 0.7521\tVal Loss: 0.8428 Acc: 0.7106\n",
      "Epoch 86/199\tTrain Loss: 0.7034 Acc: 0.7531\tVal Loss: 0.8214 Acc: 0.7202\n",
      "Epoch 87/199\tTrain Loss: 0.6953 Acc: 0.7566\tVal Loss: 0.8268 Acc: 0.7187\n",
      "Epoch 88/199\tTrain Loss: 0.6939 Acc: 0.7575\tVal Loss: 0.8156 Acc: 0.7219\n",
      "Epoch 89/199\tTrain Loss: 0.6909 Acc: 0.7579\tVal Loss: 0.8142 Acc: 0.7222\n",
      "Epoch 90/199\tTrain Loss: 0.6875 Acc: 0.7591\tVal Loss: 0.8138 Acc: 0.7231\n",
      "Epoch 91/199\tTrain Loss: 0.6890 Acc: 0.7582\tVal Loss: 0.8138 Acc: 0.7232\n",
      "Epoch 92/199\tTrain Loss: 0.6892 Acc: 0.7584\tVal Loss: 0.8164 Acc: 0.7203\n",
      "Epoch 93/199\tTrain Loss: 0.6928 Acc: 0.7568\tVal Loss: 0.8284 Acc: 0.7165\n",
      "Epoch 94/199\tTrain Loss: 0.6981 Acc: 0.7547\tVal Loss: 0.8341 Acc: 0.7143\n",
      "Epoch 95/199\tTrain Loss: 0.6998 Acc: 0.7537\tVal Loss: 0.8338 Acc: 0.7152\n",
      "Epoch 96/199\tTrain Loss: 0.7002 Acc: 0.7537\tVal Loss: 0.8365 Acc: 0.7139\n",
      "Epoch 97/199\tTrain Loss: 0.7016 Acc: 0.7524\tVal Loss: 0.8576 Acc: 0.7093\n",
      "Epoch 98/199\tTrain Loss: 0.7036 Acc: 0.7518\tVal Loss: 0.8688 Acc: 0.7008\n",
      "Epoch 99/199\tTrain Loss: 0.7037 Acc: 0.7530\tVal Loss: 0.8628 Acc: 0.7036\n",
      "Epoch 100/199\tTrain Loss: 0.6959 Acc: 0.7560\tVal Loss: 0.8544 Acc: 0.7058\n",
      "Epoch 101/199\tTrain Loss: 0.7030 Acc: 0.7526\tVal Loss: 0.9250 Acc: 0.6859\n",
      "Epoch 102/199\tTrain Loss: 0.6952 Acc: 0.7537\tVal Loss: 0.8578 Acc: 0.7108\n",
      "Epoch 103/199\tTrain Loss: 0.6879 Acc: 0.7578\tVal Loss: 0.8425 Acc: 0.7087\n",
      "Epoch 104/199\tTrain Loss: 0.6816 Acc: 0.7586\tVal Loss: 0.8326 Acc: 0.7178\n",
      "Epoch 105/199\tTrain Loss: 0.6656 Acc: 0.7663\tVal Loss: 0.8088 Acc: 0.7249\n",
      "Epoch 106/199\tTrain Loss: 0.6584 Acc: 0.7683\tVal Loss: 0.8182 Acc: 0.7213\n",
      "Epoch 107/199\tTrain Loss: 0.6564 Acc: 0.7704\tVal Loss: 0.8184 Acc: 0.7215\n",
      "Epoch 108/199\tTrain Loss: 0.6483 Acc: 0.7742\tVal Loss: 0.8038 Acc: 0.7246\n",
      "Epoch 109/199\tTrain Loss: 0.6486 Acc: 0.7741\tVal Loss: 0.8004 Acc: 0.7271\n",
      "Epoch 110/199\tTrain Loss: 0.6445 Acc: 0.7748\tVal Loss: 0.8002 Acc: 0.7264\n",
      "Epoch 111/199\tTrain Loss: 0.6457 Acc: 0.7736\tVal Loss: 0.8003 Acc: 0.7258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/199\tTrain Loss: 0.6452 Acc: 0.7760\tVal Loss: 0.8016 Acc: 0.7294\n",
      "Epoch 113/199\tTrain Loss: 0.6494 Acc: 0.7736\tVal Loss: 0.8164 Acc: 0.7198\n",
      "Epoch 114/199\tTrain Loss: 0.6541 Acc: 0.7709\tVal Loss: 0.8124 Acc: 0.7236\n",
      "Epoch 115/199\tTrain Loss: 0.6550 Acc: 0.7699\tVal Loss: 0.8397 Acc: 0.7161\n",
      "Epoch 116/199\tTrain Loss: 0.6577 Acc: 0.7692\tVal Loss: 0.8330 Acc: 0.7158\n",
      "Epoch 117/199\tTrain Loss: 0.6655 Acc: 0.7663\tVal Loss: 0.8356 Acc: 0.7187\n",
      "Epoch 118/199\tTrain Loss: 0.6699 Acc: 0.7632\tVal Loss: 0.8465 Acc: 0.7112\n",
      "Epoch 119/199\tTrain Loss: 0.6590 Acc: 0.7688\tVal Loss: 0.8449 Acc: 0.7124\n",
      "Epoch 120/199\tTrain Loss: 0.6633 Acc: 0.7665\tVal Loss: 0.8473 Acc: 0.7124\n",
      "Epoch 121/199\tTrain Loss: 0.6553 Acc: 0.7705\tVal Loss: 0.8400 Acc: 0.7151\n",
      "Epoch 122/199\tTrain Loss: 0.6529 Acc: 0.7703\tVal Loss: 0.9113 Acc: 0.6965\n",
      "Epoch 123/199\tTrain Loss: 0.6512 Acc: 0.7718\tVal Loss: 0.8278 Acc: 0.7196\n",
      "Epoch 124/199\tTrain Loss: 0.6376 Acc: 0.7764\tVal Loss: 0.8242 Acc: 0.7203\n",
      "Epoch 125/199\tTrain Loss: 0.6275 Acc: 0.7798\tVal Loss: 0.8019 Acc: 0.7286\n",
      "Epoch 126/199\tTrain Loss: 0.6196 Acc: 0.7834\tVal Loss: 0.8061 Acc: 0.7271\n",
      "Epoch 127/199\tTrain Loss: 0.6138 Acc: 0.7842\tVal Loss: 0.7963 Acc: 0.7281\n",
      "Epoch 128/199\tTrain Loss: 0.6076 Acc: 0.7871\tVal Loss: 0.7908 Acc: 0.7326\n",
      "Epoch 129/199\tTrain Loss: 0.6063 Acc: 0.7873\tVal Loss: 0.7907 Acc: 0.7318\n",
      "Epoch 130/199\tTrain Loss: 0.6063 Acc: 0.7886\tVal Loss: 0.7906 Acc: 0.7317\n",
      "Epoch 131/199\tTrain Loss: 0.6051 Acc: 0.7887\tVal Loss: 0.7907 Acc: 0.7324\n",
      "Epoch 132/199\tTrain Loss: 0.6093 Acc: 0.7876\tVal Loss: 0.7928 Acc: 0.7290\n",
      "Epoch 133/199\tTrain Loss: 0.6090 Acc: 0.7861\tVal Loss: 0.8082 Acc: 0.7290\n",
      "Epoch 134/199\tTrain Loss: 0.6117 Acc: 0.7866\tVal Loss: 0.8072 Acc: 0.7278\n",
      "Epoch 135/199\tTrain Loss: 0.6163 Acc: 0.7852\tVal Loss: 0.8132 Acc: 0.7268\n",
      "Epoch 136/199\tTrain Loss: 0.6209 Acc: 0.7811\tVal Loss: 0.8461 Acc: 0.7155\n",
      "Epoch 137/199\tTrain Loss: 0.6248 Acc: 0.7811\tVal Loss: 0.8339 Acc: 0.7170\n",
      "Epoch 138/199\tTrain Loss: 0.6296 Acc: 0.7780\tVal Loss: 0.8321 Acc: 0.7202\n",
      "Epoch 139/199\tTrain Loss: 0.6310 Acc: 0.7787\tVal Loss: 0.8532 Acc: 0.7106\n",
      "Epoch 140/199\tTrain Loss: 0.6259 Acc: 0.7796\tVal Loss: 0.8298 Acc: 0.7181\n",
      "Epoch 141/199\tTrain Loss: 0.6215 Acc: 0.7825\tVal Loss: 0.8253 Acc: 0.7221\n",
      "Epoch 142/199\tTrain Loss: 0.6140 Acc: 0.7856\tVal Loss: 0.8466 Acc: 0.7156\n",
      "Epoch 143/199\tTrain Loss: 0.6044 Acc: 0.7877\tVal Loss: 0.8211 Acc: 0.7265\n",
      "Epoch 144/199\tTrain Loss: 0.6013 Acc: 0.7896\tVal Loss: 0.8057 Acc: 0.7259\n",
      "Epoch 145/199\tTrain Loss: 0.5934 Acc: 0.7925\tVal Loss: 0.8300 Acc: 0.7221\n",
      "Epoch 146/199\tTrain Loss: 0.5873 Acc: 0.7915\tVal Loss: 0.7968 Acc: 0.7293\n",
      "Epoch 147/199\tTrain Loss: 0.5787 Acc: 0.7982\tVal Loss: 0.8017 Acc: 0.7322\n",
      "Epoch 148/199\tTrain Loss: 0.5757 Acc: 0.7994\tVal Loss: 0.7890 Acc: 0.7340\n",
      "Epoch 149/199\tTrain Loss: 0.5721 Acc: 0.8004\tVal Loss: 0.7863 Acc: 0.7349\n",
      "Epoch 150/199\tTrain Loss: 0.5717 Acc: 0.8020\tVal Loss: 0.7863 Acc: 0.7355\n",
      "Epoch 151/199\tTrain Loss: 0.5700 Acc: 0.8022\tVal Loss: 0.7862 Acc: 0.7360\n",
      "Epoch 152/199\tTrain Loss: 0.5700 Acc: 0.8009\tVal Loss: 0.7902 Acc: 0.7347\n",
      "Epoch 153/199\tTrain Loss: 0.5756 Acc: 0.7994\tVal Loss: 0.7888 Acc: 0.7344\n",
      "Epoch 154/199\tTrain Loss: 0.5807 Acc: 0.7972\tVal Loss: 0.7987 Acc: 0.7328\n",
      "Epoch 155/199\tTrain Loss: 0.5818 Acc: 0.7961\tVal Loss: 0.8491 Acc: 0.7193\n",
      "Epoch 156/199\tTrain Loss: 0.5937 Acc: 0.7904\tVal Loss: 0.8220 Acc: 0.7209\n",
      "Epoch 157/199\tTrain Loss: 0.5975 Acc: 0.7884\tVal Loss: 0.8064 Acc: 0.7278\n",
      "Epoch 158/199\tTrain Loss: 0.5994 Acc: 0.7894\tVal Loss: 0.8432 Acc: 0.7184\n",
      "Epoch 159/199\tTrain Loss: 0.5921 Acc: 0.7938\tVal Loss: 0.8516 Acc: 0.7192\n",
      "Epoch 160/199\tTrain Loss: 0.5927 Acc: 0.7909\tVal Loss: 0.8333 Acc: 0.7230\n",
      "Epoch 161/199\tTrain Loss: 0.5965 Acc: 0.7885\tVal Loss: 0.8897 Acc: 0.7086\n",
      "Epoch 162/199\tTrain Loss: 0.5886 Acc: 0.7931\tVal Loss: 0.8052 Acc: 0.7271\n",
      "Epoch 163/199\tTrain Loss: 0.5754 Acc: 0.7995\tVal Loss: 0.8168 Acc: 0.7263\n",
      "Epoch 164/199\tTrain Loss: 0.5670 Acc: 0.8011\tVal Loss: 0.7933 Acc: 0.7317\n",
      "Epoch 165/199\tTrain Loss: 0.5585 Acc: 0.8057\tVal Loss: 0.7984 Acc: 0.7302\n",
      "Epoch 166/199\tTrain Loss: 0.5515 Acc: 0.8091\tVal Loss: 0.7942 Acc: 0.7357\n",
      "Epoch 167/199\tTrain Loss: 0.5451 Acc: 0.8111\tVal Loss: 0.7926 Acc: 0.7355\n",
      "Epoch 168/199\tTrain Loss: 0.5432 Acc: 0.8119\tVal Loss: 0.7889 Acc: 0.7363\n",
      "Epoch 169/199\tTrain Loss: 0.5387 Acc: 0.8133\tVal Loss: 0.7855 Acc: 0.7368\n",
      "Epoch 170/199\tTrain Loss: 0.5362 Acc: 0.8141\tVal Loss: 0.7854 Acc: 0.7380\n",
      "Epoch 171/199\tTrain Loss: 0.5388 Acc: 0.8132\tVal Loss: 0.7854 Acc: 0.7369\n",
      "Epoch 172/199\tTrain Loss: 0.5416 Acc: 0.8118\tVal Loss: 0.7887 Acc: 0.7347\n",
      "Epoch 173/199\tTrain Loss: 0.5427 Acc: 0.8109\tVal Loss: 0.7921 Acc: 0.7344\n",
      "Epoch 174/199\tTrain Loss: 0.5544 Acc: 0.8070\tVal Loss: 0.7941 Acc: 0.7333\n",
      "Epoch 175/199\tTrain Loss: 0.5510 Acc: 0.8068\tVal Loss: 0.8296 Acc: 0.7226\n",
      "Epoch 176/199\tTrain Loss: 0.5565 Acc: 0.8054\tVal Loss: 0.8167 Acc: 0.7290\n",
      "Epoch 177/199\tTrain Loss: 0.5664 Acc: 0.8007\tVal Loss: 0.8055 Acc: 0.7332\n",
      "Epoch 178/199\tTrain Loss: 0.5689 Acc: 0.8018\tVal Loss: 0.8365 Acc: 0.7214\n",
      "Epoch 179/199\tTrain Loss: 0.5711 Acc: 0.7989\tVal Loss: 0.8426 Acc: 0.7204\n",
      "Epoch 180/199\tTrain Loss: 0.5634 Acc: 0.8006\tVal Loss: 0.8410 Acc: 0.7225\n",
      "Epoch 181/199\tTrain Loss: 0.5711 Acc: 0.7995\tVal Loss: 0.8496 Acc: 0.7184\n",
      "Epoch 182/199\tTrain Loss: 0.5578 Acc: 0.8037\tVal Loss: 0.8608 Acc: 0.7182\n",
      "Epoch 183/199\tTrain Loss: 0.5458 Acc: 0.8094\tVal Loss: 0.8328 Acc: 0.7209\n",
      "Epoch 184/199\tTrain Loss: 0.5441 Acc: 0.8079\tVal Loss: 0.8128 Acc: 0.7298\n",
      "Epoch 185/199\tTrain Loss: 0.5318 Acc: 0.8140\tVal Loss: 0.8253 Acc: 0.7256\n",
      "Epoch 186/199\tTrain Loss: 0.5244 Acc: 0.8199\tVal Loss: 0.8010 Acc: 0.7326\n",
      "Epoch 187/199\tTrain Loss: 0.5204 Acc: 0.8199\tVal Loss: 0.7959 Acc: 0.7341\n",
      "Epoch 188/199\tTrain Loss: 0.5141 Acc: 0.8222\tVal Loss: 0.7892 Acc: 0.7368\n",
      "Epoch 189/199\tTrain Loss: 0.5103 Acc: 0.8233\tVal Loss: 0.7898 Acc: 0.7360\n",
      "Epoch 190/199\tTrain Loss: 0.5077 Acc: 0.8257\tVal Loss: 0.7889 Acc: 0.7357\n",
      "Epoch 191/199\tTrain Loss: 0.5074 Acc: 0.8249\tVal Loss: 0.7897 Acc: 0.7366\n",
      "Epoch 192/199\tTrain Loss: 0.5120 Acc: 0.8234\tVal Loss: 0.7950 Acc: 0.7323\n",
      "Epoch 193/199\tTrain Loss: 0.5168 Acc: 0.8205\tVal Loss: 0.7951 Acc: 0.7367\n",
      "Epoch 194/199\tTrain Loss: 0.5174 Acc: 0.8209\tVal Loss: 0.8020 Acc: 0.7334\n",
      "Epoch 195/199\tTrain Loss: 0.5278 Acc: 0.8150\tVal Loss: 0.8096 Acc: 0.7296\n",
      "Epoch 196/199\tTrain Loss: 0.5311 Acc: 0.8141\tVal Loss: 0.8279 Acc: 0.7286\n",
      "Epoch 197/199\tTrain Loss: 0.5338 Acc: 0.8124\tVal Loss: 0.8522 Acc: 0.7219\n",
      "Epoch 198/199\tTrain Loss: 0.5423 Acc: 0.8081\tVal Loss: 0.8234 Acc: 0.7278\n",
      "Epoch 199/199\tTrain Loss: 0.5394 Acc: 0.8110\tVal Loss: 0.8206 Acc: 0.7294\n",
      "\n",
      "Best val Acc: 0.738000\n",
      "\n",
      "Elapsed time - : 9min 30s\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"batch_size\": 2048,\n",
    "    \"epochs\": 100,\n",
    "    \"lr\": 0.001,\n",
    "    \"momentum\": 0.9,\n",
    "}\n",
    "\n",
    "image_datasets, dataset_sizes = init_datasets(data_transforms=data_transforms)\n",
    "dataloaders = init_dataloaders(params[\"batch_size\"], image_datasets=image_datasets)\n",
    "model = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                                            lr=params[\"lr\"])\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer=optimizer,\n",
    "                                                                              T_max=10,  # Maximum number of iterations\n",
    "                                                                              eta_min=0,  # 最小学習率\n",
    "                                                                              last_epoch=-1)  # The index of last epoch\n",
    "\n",
    "model = train(model, criterion, optimizer, scheduler, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of\tplane:\t77 %\n",
      "Accuracy of\t  car:\t84 %\n",
      "Accuracy of\t bird:\t63 %\n",
      "Accuracy of\t  cat:\t53 %\n",
      "Accuracy of\t deer:\t71 %\n",
      "Accuracy of\t  dog:\t62 %\n",
      "Accuracy of\t frog:\t82 %\n",
      "Accuracy of\thorse:\t79 %\n",
      "Accuracy of\t ship:\t82 %\n",
      "Accuracy of\ttruck:\t81 %\n"
     ]
    }
   ],
   "source": [
    "calc_acc(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(\"original_Adam_CosineAnnealing_batchsize2048_epoch200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"batch_size\": 1024,\n",
    "    \"epochs\": 50,\n",
    "    \"lr\": 0.001,\n",
    "    \"momentum\": 0.9,\n",
    "}\n",
    "\n",
    "dataloaders = init_dataloaders(batch_size)\n",
    "model = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                                            lr=params[\"lr\"])\n",
    "scheduler = lr_scheduler.StepLR(optimizer=optimizer,\n",
    "                                                     step_size=10,\n",
    "                                                     gamma=0.9)\n",
    "\n",
    "model = train(model, criterion, optimizer, scheduler, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_acc(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning ResNet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "}\n",
    "\n",
    "image_datasets, dataset_sizes = init_datasets(data_transforms)\n",
    "dataloaders = init_dataloaders(params[\"batch_size\"], image_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del dataloaders; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 0/9\tLoss Train: 1.5141 Val: 1.2037\tAcc Train: 0.4587 Val: 0.5722\n",
      "Epoch 1/9\tLoss Train: 1.1095 Val: 1.0641\tAcc Train: 0.6092 Val: 0.6336\n",
      "Epoch 2/9\tLoss Train: 0.9595 Val: 0.9358\tAcc Train: 0.6643 Val: 0.6711\n",
      "Epoch 3/9\tLoss Train: 0.8530 Val: 0.8154\tAcc Train: 0.7047 Val: 0.7114\n",
      "Epoch 4/9\tLoss Train: 0.7749 Val: 0.7330\tAcc Train: 0.7300 Val: 0.7451\n",
      "Epoch 5/9\tLoss Train: 0.6925 Val: 0.6436\tAcc Train: 0.7598 Val: 0.7752\n",
      "Epoch 6/9\tLoss Train: 0.6300 Val: 0.6113\tAcc Train: 0.7813 Val: 0.7872\n",
      "Epoch 7/9\tLoss Train: 0.5681 Val: 0.5421\tAcc Train: 0.8039 Val: 0.8082\n",
      "Epoch 8/9\tLoss Train: 0.5192 Val: 0.5410\tAcc Train: 0.8194 Val: 0.8152\n",
      "Epoch 9/9\tLoss Train: 0.4948 Val: 0.5309\tAcc Train: 0.8277 Val: 0.8172\n",
      "\n",
      "Best val Acc: 0.817200\n",
      "\n",
      "Elapsed time - : 25min 23s\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 10,\n",
    "    \"lr\": 0.001,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 1e-5,\n",
    "}\n",
    "\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "}\n",
    "\n",
    "image_datasets, dataset_sizes = init_datasets(data_transforms)\n",
    "dataloaders = init_dataloaders(params[\"batch_size\"], image_datasets)\n",
    "\n",
    "# Load pretrained model\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.conv1 = nn.Conv2d(in_channels=3,\n",
    "                                                    out_channels=64,\n",
    "                                                    kernel_size=7,\n",
    "                                                    stride=2,\n",
    "                                                    padding=3,\n",
    "                                                    bias=False)\n",
    "model_ft.fc = nn.Linear(num_ftrs, 10)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = generate_optimizer(model_ft, params, \"Adam\")\n",
    "scheduler = generate_scheduler(optimizer, \"cos\")\n",
    "\n",
    "model_ft = train(model_ft, criterion, optimizer, scheduler, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of\tplane:\t82 %\n",
      "Accuracy of\t  car:\t87 %\n",
      "Accuracy of\t bird:\t77 %\n",
      "Accuracy of\t  cat:\t71 %\n",
      "Accuracy of\t deer:\t81 %\n",
      "Accuracy of\t  dog:\t80 %\n",
      "Accuracy of\t frog:\t87 %\n",
      "Accuracy of\thorse:\t82 %\n",
      "Accuracy of\t ship:\t86 %\n",
      "Accuracy of\ttruck:\t87 %\n"
     ]
    }
   ],
   "source": [
    "calc_acc(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model_ft, \"resnet18_Adam_CosineAnnealing_batchsize32_epoch10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "859"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 0/14\tLoss Train: 1.7771 Val: 1.5793\tAcc Train: 0.3616 Val: 0.4458\n",
      "Epoch 1/14\tLoss Train: 1.5004 Val: 1.4649\tAcc Train: 0.4706 Val: 0.4807\n",
      "Epoch 2/14\tLoss Train: 1.4187 Val: 1.3718\tAcc Train: 0.5001 Val: 0.5164\n",
      "Epoch 3/14\tLoss Train: 1.3660 Val: 1.3702\tAcc Train: 0.5180 Val: 0.5216\n",
      "Epoch 4/14\tLoss Train: 1.3098 Val: 1.3126\tAcc Train: 0.5412 Val: 0.5424\n",
      "Epoch 5/14\tLoss Train: 1.2748 Val: 1.2948\tAcc Train: 0.5519 Val: 0.5417\n",
      "Epoch 6/14\tLoss Train: 1.2252 Val: 1.1718\tAcc Train: 0.5700 Val: 0.5858\n",
      "Epoch 7/14\tLoss Train: 1.1644 Val: 1.1419\tAcc Train: 0.5926 Val: 0.5955\n",
      "Epoch 8/14\tLoss Train: 1.1502 Val: 1.1211\tAcc Train: 0.6002 Val: 0.5999\n",
      "Epoch 9/14\tLoss Train: 1.1430 Val: 1.1217\tAcc Train: 0.6021 Val: 0.6100\n",
      "Epoch 10/14\tLoss Train: 1.1350 Val: 1.1273\tAcc Train: 0.6051 Val: 0.6119\n",
      "Epoch 11/14\tLoss Train: 1.1264 Val: 1.1102\tAcc Train: 0.6079 Val: 0.6108\n",
      "Epoch 12/14\tLoss Train: 1.1245 Val: 1.1028\tAcc Train: 0.6087 Val: 0.6121\n",
      "Epoch 13/14\tLoss Train: 1.1180 Val: 1.1019\tAcc Train: 0.6112 Val: 0.6146\n",
      "Epoch 14/14\tLoss Train: 1.1105 Val: 1.0916\tAcc Train: 0.6148 Val: 0.6179\n",
      "\n",
      "Best val Acc: 0.617900\n",
      "\n",
      "Elapsed time - : 24min 16s\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 15,\n",
    "    \"lr\": 0.001,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 1e-5,\n",
    "}\n",
    "\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "}\n",
    "\n",
    "image_datasets, dataset_sizes = init_datasets(data_transforms)\n",
    "dataloaders = init_dataloaders(params[\"batch_size\"], image_datasets)\n",
    "\n",
    "# Load pretrained model\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.conv1 = nn.Conv2d(in_channels=3,\n",
    "                                                    out_channels=64,\n",
    "                                                    kernel_size=7,\n",
    "                                                    stride=2,\n",
    "                                                    padding=3,\n",
    "                                                    bias=False)\n",
    "model_ft.fc = nn.Linear(num_ftrs, 10)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = generate_optimizer(model_ft, params, \"Adam\")\n",
    "scheduler = generate_scheduler(optimizer, \"step\")\n",
    "\n",
    "model_ft = train(model_ft, criterion, optimizer, scheduler, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model_ft, \"resnet18_freeze_Adam_StepLR_batchsize128_epoch15_lr1e-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 0/9\tLoss Train: 1.3554 Val: 1.1196\tAcc Train: 0.5135 Val: 0.6026\n",
      "Epoch 1/9\tLoss Train: 1.3158 Val: 1.1093\tAcc Train: 0.5382 Val: 0.6000\n",
      "Epoch 2/9\tLoss Train: 1.3008 Val: 1.1036\tAcc Train: 0.5443 Val: 0.6041\n",
      "Epoch 3/9\tLoss Train: 1.2982 Val: 1.0775\tAcc Train: 0.5450 Val: 0.6162\n",
      "Epoch 4/9\tLoss Train: 1.2704 Val: 1.0930\tAcc Train: 0.5524 Val: 0.6141\n",
      "Epoch 5/9\tLoss Train: 1.2499 Val: 1.0476\tAcc Train: 0.5594 Val: 0.6311\n",
      "Epoch 6/9\tLoss Train: 1.2265 Val: 1.0747\tAcc Train: 0.5628 Val: 0.6173\n",
      "Epoch 7/9\tLoss Train: 1.2105 Val: 1.0547\tAcc Train: 0.5689 Val: 0.6312\n",
      "Epoch 8/9\tLoss Train: 1.1885 Val: 1.0487\tAcc Train: 0.5788 Val: 0.6353\n",
      "Epoch 9/9\tLoss Train: 1.1955 Val: 1.0453\tAcc Train: 0.5727 Val: 0.6346\n",
      "\n",
      "Best val Acc: 0.635300\n",
      "\n",
      "Elapsed time - : 44min 48s\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 10,\n",
    "    \"lr\": 0.001,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 1e-5,\n",
    "}\n",
    "\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "}\n",
    "\n",
    "image_datasets, dataset_sizes = init_datasets(data_transforms)\n",
    "dataloaders = init_dataloaders(params[\"batch_size\"], image_datasets)\n",
    "\n",
    "# Load pretrained model\n",
    "model_ft = models.vgg16(pretrained=True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = model_ft.classifier[6].in_features\n",
    "model_ft.classifier[6] = nn.Linear(num_ftrs, 10)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = generate_optimizer(model_ft, params, \"Adam\")\n",
    "scheduler = generate_scheduler(optimizer, \"cos\")\n",
    "\n",
    "model_ft = train(model_ft, criterion, optimizer, scheduler, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model_ft, \"vgg16_freeze_Adam_CosineAnnealing_batchsize32_epoch10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(dataloader[TRAIN], 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model_ft(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        exp_lr_scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % log_interval == (log_interval - 1):\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tRunningLoss: {:.3f}\".format(\n",
    "                epoch, batch_idx * len(inputs), len(dataloader[TRAIN].dataset),\n",
    "                100. * batch_idx / len(dataloader[TRAIN]), loss.item(), running_loss / log_interval\n",
    "            ))\n",
    "            running_loss = 0.0\n",
    "\n",
    "display_formatted_time(time.time() - since)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for (inputs, labels) in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_ft(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print(\"Accuracy of the network on the 10000 test images: %d %%\" % (100 * correct / total))\n",
    "\n",
    "display_formatted_time(time.time() - since)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
