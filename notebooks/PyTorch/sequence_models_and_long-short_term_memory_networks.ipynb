{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence Models and Long-Short Term Memory Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x115853110>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM's in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Inputs ---\n",
      "[tensor([[-0.1473,  0.3482,  1.1371]]),\n",
      " tensor([[-0.3339, -1.4724,  0.7296]]),\n",
      " tensor([[-0.1312, -0.6368,  1.0429]]),\n",
      " tensor([[ 0.4903,  1.0318, -0.5989]]),\n",
      " tensor([[ 1.6015, -1.0735, -1.2173]])]\n",
      "\n",
      "--- hidden ---\n",
      "(tensor([[[ 0.6472, -0.0412, -0.1775]]]),\n",
      " tensor([[[-0.5000,  0.8673, -0.2732]]]))\n",
      "\n",
      "--- out ---\n",
      "tensor([[[-0.1077,  0.0289, -0.0487]]], grad_fn=<CatBackward>)\n",
      "\n",
      "--- hidden ---\n",
      "(tensor([[[-0.1077,  0.0289, -0.0487]]], grad_fn=<ViewBackward>),\n",
      " tensor([[[-0.1439,  0.1426, -0.2563]]], grad_fn=<ViewBackward>))\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 3)  # Input dim: 3, Output dim: 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # 特徴3つ持った length 5 のシーケンス\n",
    "print(\"--- Inputs ---\")\n",
    "pprint(inputs)\n",
    "print()\n",
    "\n",
    "# Initialize the hidden state\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "                  torch.randn(1, 1, 3))\n",
    "print(\"--- hidden ---\")\n",
    "pprint(hidden)\n",
    "print()\n",
    "\n",
    "for i in inputs:\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "\n",
    "print(\"--- out ---\")\n",
    "pprint(out)\n",
    "print()\n",
    "print(\"--- hidden ---\")\n",
    "pprint(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1473,  0.3482,  1.1371]])\n",
      "torch.Size([1, 3])\n",
      "tensor([[[-0.1473,  0.3482,  1.1371]]])\n",
      "torch.Size([1, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "pprint(inputs[0])\n",
    "print(inputs[0].shape)\n",
    "pprint(inputs[0].view(1, 1, -1))\n",
    "print(inputs[0].view(1, 1, -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1180,  0.0690, -0.4030]],\n",
      "\n",
      "        [[-0.2783,  0.0456, -0.2278]],\n",
      "\n",
      "        [[-0.3332,  0.0701, -0.2882]],\n",
      "\n",
      "        [[-0.2436,  0.0887, -0.1496]],\n",
      "\n",
      "        [[-0.0298,  0.0178, -0.0704]]], grad_fn=<CatBackward>)\n",
      "(tensor([[[-0.0298,  0.0178, -0.0704]]], grad_fn=<ViewBackward>),\n",
      " tensor([[[-0.0399,  0.0882, -0.3806]]], grad_fn=<ViewBackward>))\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.cat(tuple(inputs)).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "pprint(out)\n",
    "pprint(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: An LSTM for Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "センテンスに含まれる単語に品詞タグを付ける"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "w: 単語 ∈ V (Vocab)\n",
    "Input sentence: w1, w2, ..., wM\n",
    "T: tag set\n",
    "yi: wi のtag\n",
    "yi^: wi のtag予測値\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Everybody': 5,\n",
      " 'The': 0,\n",
      " 'apple': 4,\n",
      " 'ate': 2,\n",
      " 'book': 8,\n",
      " 'dog': 1,\n",
      " 'read': 6,\n",
      " 'that': 7,\n",
      " 'the': 3}\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "word_to_ix = {}\n",
    "\n",
    "# 学習データから1センテンスずつ読込\n",
    "for sent, tags in training_data:\n",
    "    \n",
    "    # センテンスから1単語ずつ読込\n",
    "    for word in sent:\n",
    "        \n",
    "        # word_to_ix に単語が含まれていなかったら新しいidxを追加\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            \n",
    "pprint(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM は単語埋め込みを入力としてとり、隠れ状態とその次元を出力する\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Linear layer では隠れ状態スペースからタグスペースへの写像を行う\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # 初期状態では隠れ状態を保持していない。\n",
    "        # (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                     torch.zeros(1, 1, self.hidden_dim))\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)  # 埋め込みベクトル?\n",
    "\n",
    "        '''\n",
    "        print(\"--- embeds\")\n",
    "        pprint(embeds)\n",
    "        print(\"--- embeds.view()\")\n",
    "        pprint(embeds.view(len(sentence), 1, -1))\n",
    "        '''\n",
    "\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== タグスコア\n",
      "tensor([[-0.8692, -1.2669, -1.2073],\n",
      "        [-0.9927, -1.2815, -1.0447],\n",
      "        [-0.9718, -1.1951, -1.1428],\n",
      "        [-0.9181, -1.2389, -1.1679],\n",
      "        [-0.9034, -1.2687, -1.1596]])\n"
     ]
    }
   ],
   "source": [
    "# 出力の要素 i, j  は word i に対する tag j のスコア\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(\"=== タグスコア\")\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(300):\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. 勾配初期化\n",
    "        model.zero_grad()\n",
    "\n",
    "        # LSTM の隠れ状態初期化\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. Input の準備。単語インデックスからなる torch.tensor 型。\n",
    "        # 予め、単語をインデックスにマップするものを作成しておき、\n",
    "        # さらにそれを使って、センテンスをテンソルに変換する。\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Optim\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "タグスコア見る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- tag_scores\n",
      "tensor([[-0.1917, -1.8349, -4.2134],\n",
      "        [-3.8420, -0.0579, -3.3582],\n",
      "        [-2.9986, -3.9851, -0.0709],\n",
      "        [-0.0594, -3.2447, -3.9816],\n",
      "        [-2.4667, -0.0944, -5.2584]])\n",
      "--- max tag_scores\n",
      "(tensor([-0.1917, -0.0579, -0.0709, -0.0594, -0.0944]), tensor([0, 1, 2, 0, 1]))\n",
      "\n",
      "True:  ['The', 'dog', 'ate', 'the', 'apple']\n",
      "Preds:  ['DET', 'NN', 'V', 'DET', 'NN']\n"
     ]
    }
   ],
   "source": [
    "ix_to_tag = {v: k for k, v, in tag_to_ix.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    \n",
    "    pred_tags = torch.max(tag_scores, dim=1)[1].tolist()\n",
    "    \n",
    "    # The sentence is \"the dog ate the apple\".\n",
    "    print(\"--- tag_scores\")\n",
    "    pprint(tag_scores)\n",
    "    print(\"--- max tag_scores\")\n",
    "    print(torch.max(tag_scores, dim=1))\n",
    "    print()\n",
    "    print(\"True: \", training_data[0][0])\n",
    "    print(\"Preds: \", [ix_to_tag[pred] for pred in pred_tags])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
